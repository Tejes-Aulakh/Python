{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tejes-Aulakh/Python/blob/main/Intro_to_AI_KMC_Iris_DataSet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Edited content:\n",
        "\n",
        "# Iris Clustering Using K-Means Clustering\n",
        "\n",
        "The Iris dataset is a classic dataset in the field of machine learning and statistics. It is often used for benchmarking algorithms and learning how to handle data. Here are the key details about the Iris dataset:\n",
        "\n",
        "## Description:\n",
        "\n",
        "The Iris dataset consists of 150 samples of iris flowers, each described by four features. The dataset contains three different species of iris flowers: *Iris setosa*, *Iris versicolor*, and *Iris virginica*. Each species has 50 samples.\n",
        "\n",
        "## Features:\n",
        "\n",
        "The dataset includes the following four features (all measured in centimetres):\n",
        "\n",
        "* Sepal length\n",
        "* Sepal width\n",
        "* Petal length\n",
        "* Petal width\n",
        "\n",
        "These features are the dimensions of the flowers' sepals and petals.\n",
        "\n",
        "## Target:\n",
        "\n",
        "The target variable is the species of the iris flower, which can take one of three possible values:\n",
        "\n",
        "* *Iris setosa*\n",
        "* *Iris versicolor*\n",
        "* *Iris virginica*\n",
        "\n",
        "## Data Structure:\n",
        "\n",
        "* Number of samples: 150\n",
        "* Number of features: 4\n",
        "* Number of classes: 3 (*Iris setosa*, *Iris versicolor*, *Iris virginica*)\n",
        "\n",
        "\n",
        "## Example Data:\n",
        "Here is a sample from the dataset:\n",
        "\n",
        "\n",
        "| Sepal Length (cm)\t| Sepal Width (cm) | Petal Length (cm) |\tPetal Width (cm) |\tSpecies |\n",
        "|---|----|---|---|---|\n",
        "| 5.1 |\t3.5 |\t1.4 |\t0.2 |\tIris setosa\n",
        "| 7.0 |\t3.2 |\t4.7 |\t1.4 |\tIris versicolor\n",
        "| 6.3 |\t3.3 |\t6.0 |\t2.5 |\tIris virginica\n"
      ],
      "metadata": {
        "id": "s9NZOJcVApsL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WdVepmSiy1LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries\n",
        "Python includes a large number of libraries, either pre-installed or available through package managers such as pip, to support machine learning. The four libraries we will be using are NumPy, pandas, scikit-learn, and Matplotlib. It may be useful to spend some time reading about these libraries."
      ],
      "metadata": {
        "id": "QlAiB457B7eE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "nFI1SYYWAWnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load a Dataset\n",
        "scikit-learn provides a one-shot (single-line) load routine for the Iris dataset, allowing us to load data that we know is already sanitized (checked and cleaned of errors) and split it into data and target variables, `x` and `y`."
      ],
      "metadata": {
        "id": "sOQZtWexB6VA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "x = iris.data\n",
        "y = iris.target"
      ],
      "metadata": {
        "id": "17UpgiLoAcSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standardise the data features\n",
        "In our example, the standardized features are the four measurements from the Iris dataset (sepal length, sepal width, petal length, and petal width) that have been scaled to have a mean of 0 and a standard deviation of 1. Standardizing features is a common preprocessing step in machine learning, helping to ensure that each feature contributes equally to the analysis and improving the performance of many algorithms.\n",
        "\n",
        "In the provided script, we used the `StandardScaler` from scikit-learn to standardize the features. Here’s the relevant part of the script:\n",
        "\n",
        "```python\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "```\n",
        "\n",
        "After standardisation, the features will have the following properties:\n",
        "- Mean: 0\n",
        "- Standard Deviation: 1\n"
      ],
      "metadata": {
        "id": "JA00NwdwCliT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardise the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(x)"
      ],
      "metadata": {
        "id": "EWk6X6lZAeWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform the k-means clustering\n",
        "We need to actually run the algorithm. In this case, we know there are three possible options for the irises—the three species of the flower—so we set the number of clusters to 3. The choice of 42 for the random state is arbitrary, but by setting it instead of using a truly random number, we can introduce reproducibility and consistency. Your clusters should match ours. Once we've set up our specifications, we can create the clusters on our scaled data using the fit method."
      ],
      "metadata": {
        "id": "VJh9JG7gDcq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans.fit(X_scaled)"
      ],
      "metadata": {
        "id": "gyLo_aKbAiZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Map cluster labels to species names\n",
        "Now we have our clusters, it's all about making the data readable by humans. We can create a data frame in pandas from the data. A data frame is like a table in Excel, though a lot more powerful. In this case we willmap each of the iris types and then apply the names to the clusters."
      ],
      "metadata": {
        "id": "VOj-WIuwE74S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "species = np.array(['Iris setosa', 'Iris versicolor', 'Iris virginica'])\n",
        "cluster_mapping = {}\n",
        "for i in range(3):\n",
        "    # Find the most common species in each cluster\n",
        "    cluster_species = y[kmeans.labels_ == i]\n",
        "    most_common_species = np.bincount(cluster_species).argmax()\n",
        "    cluster_mapping[i] = species[most_common_species]"
      ],
      "metadata": {
        "id": "TArN2kyRAkP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot the results\n",
        "Of course, we want to see the results using matplotlib's handy graph plotting libraries. In this case, we will view the first two fields of the dataset pesky computer screens and their 2D displays). To change what is displayed, the numbers in the two `X_scaled[kmeans.labels_ == i, n],` parameters on line 37 would need to be changed, along with the appropriate label of course."
      ],
      "metadata": {
        "id": "E7NTk1GeFLip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot with labeled clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "colors = ['r', 'g', 'b']\n",
        "markers = ['o', 's', 'D']\n",
        "\n",
        "for i in range(3):\n",
        "    plt.scatter(X_scaled[kmeans.labels_ == i, 0], X_scaled[kmeans.labels_ == i, 1],\n",
        "                c=colors[i], marker=markers[i], label=cluster_mapping[i], edgecolor='k')\n",
        "\n",
        "plt.title('K-Means Clustering of Iris Dataset')\n",
        "plt.xlabel('Standardized Sepal Length')\n",
        "plt.ylabel('Standardized Sepal Width')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S3CLxgu8AmAt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}